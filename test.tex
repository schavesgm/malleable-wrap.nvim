\chapter{Short introduction to neural networks}\label{chap:neural_networks}
During the last decade, the theoretical developments and practical applications of neural networks to both
academic and industry problems have largely increased. Their outstanding impact in most scientific fields can
be understood in terms of their adaptability, flexibility, expressiveness, and computational efficiency.
Additionally, the fact that collecting, manipulating, and storing large amounts of data has become easier and
more affordable in the last few years generates an environment in which neural networks can thrive. As a
result, nowadays, neural network models are applied to a variety of complex tasks, such as: standard
classification and regression problems~\cite{wan1990neural, feraud2002methodology}, reinforcement
learning~\cite{ReinforcementOverview, ReinforcementOverview2}, graph theory~\cite{Wu_2021}, and natural
language processing~\cite{NLPIntro, NLPIntro2}. The list of possible tasks to which neural networks can be
applied is continuously growing as more research is performed to understand, and enhance, their inherent
properties. In addition, the increase in computational power of graphical processing units (GPUs), and, more
recently, the arrival of dedicated tensor processing units (TPUs), combined with the availability of highly
optimised open-source software libraries~\cite{PyTorch, Keras, tensorflow2015} allows an almost
straightforward application of most state-of-the-art neural network models to computationally demanding tasks.

This chapter is divided into two main sections. The first one contains a short introduction to the field of
neural networks where no previous knowledge about machine learning nor neural networks is assumed. In this
section, we also discuss how to train efficiently neural network models. The second section contains a small
introduction to convolutional neural networks, which are employed in Chapter~(\ref{chap:spectrec}).

\section{Introducing standard neural networks}
The roots of neural network models can be traced to the decade of the 1950s, when researchers were trying to
mathematically model decision-making. One of the simplest decision-making models is the
perceptron~\cite{rosenblatt1958perceptron}, which is a simplified model of the behaviour of neurons in the
brain. The perceptron unit proposes a deterministic output, also called the neuron's action, depending on some
input variables.

Figure~(\ref{fig:neural_networks/perceptron}) contains a diagram showing a perceptron unit defined on an input
of $4$ dimensions, labelled $x = (x_1, x_2, x_3, x_4)$, and a $1$-dimensional output, labelled $y$. Although
the output in Figure~(\ref{fig:neural_networks/perceptron}) is $1$-dimensional, perceptrons are also able to
handle multidimensional outputs.

\begin{figure}[H]
	\caption{
        Diagram representing a perceptron unit. A vector of inputs $x = (x_1, x_2, x_3, x_4)$ is
        processed by the perceptron in order to produce an output signal $y$.
    }\label{fig:neural_networks/perceptron}
	\centering
    \begin{tikzpicture}

        % Input
        \foreach \x in {1, ..., 4} {
            \draw node at (0.0, -\x * 1.25) [node, fill=TikzGreen!30] (input_\x) 
                {$\color{TikzBlue} x_{\x}$};
        };

        % Perceptron node
        \draw node at (3.0, -2.5 * 1.25) [node, fill=TikzOrange!30] (perceptron) {};

        % Output node
        \draw node at (6.0, -2.5 * 1.25) [node, fill=TikzCream] (output) {$\color{TikzBlue} y$};

        % Make the connections in the model
        \foreach \i in {1, ..., 4} { \draw [arrow] (input_\i) to (perceptron); };
        \draw [arrow] (perceptron) to (output);
    \end{tikzpicture}
\end{figure}

To see how the perceptron unit can be used to automate decision making, we focus on a particular task. Imagine
that we wanted to decide whether we should go to the office or work from home one particular morning. In this
simple case, the decision space is binary: either we go, corresponding to an \textit{activated} output, $y =
1$; or we stay at home, corresponding to a \textit{turned off} outcome, $y = 0$. We assume that the process of
making a decision is not irrational: we deliberately make a decision based on some reasonable conditions, and,
moreover, the results are consistent for a fixed input. In addition, suppose we knew that only $4$ independent
variables affect our final decision. For instance, those $4$ variables could represent: the distance between
our home and the office; the weather that morning; the amount of work we need to do that precise day; and the
current risk level of COVID-19 in our area. Note that all input variables should be mappable to a numeric
value.

It is clear that each input variable should have a different impact in our final decision. For example, a high
risk level of COVID-19 should encourage us to stay at home independently of all the other conditions. To model
the importance of each input, we use some weights $W$. Each independent weight acts on a particular input
variable, weighting its relevance in the final decision. The dimensions of the weights are not important, as
we are not trying to measure a physically relevant quantity. However, we should normalise the inputs in order
to avoid including artificial numerical bias towards any input variable. The scales of the inputs should be
comparable so as not to artificially benefit any input variable. This is a constant requirement in machine
learning, inputs should always be scaled appropriately.

Given some inputs $x = (x_1, \dots, x_n)$ and some weights $W = (w_1, \dots, w_n)$, the simplest non-trivial
mathematical model that can be used to make decisions is
\begin{equation}
    y = W^T \circ x,
    \label{eq:neural_networks/perc_unit}
\end{equation}
where $W^T$ denotes the transpose of $W$, and $\circ$ represents the standard matrix multiplication operator.
The output of eq.~(\ref{eq:neural_networks/perc_unit}) is a real number, however, in our particular task, the
decision space is binary. In order to transform the real output into a binary variable, we can introduce a
threshold $b$, which implies that the mathematical model of the perceptron is transformed to
\begin{equation}
    y = \left\{
	\begin{array}{ll}
		0  & \mbox{if } y < b \\
		1  & \mbox{if } y \geq b \\
	\end{array} \right.\label{eq:neural_networks/threshold}
\end{equation}
The threshold is called the bias of the model, as it represents our own personal bias in the task in question:
some people might enjoy going to the office more than others, so they will accept smaller values of $y$ as an
activated output. The bias defines the boundary that dictates whether $W^T \circ x$ is regarded as an
activated or turned off outcome. The standard way of modelling a binary perceptron unit with a scalar bias $b$
is:
\begin{equation}
    y = \left\{
        \begin{array}{ll}
            0  & (\mbox{if } W^T \circ x - b) < 0 \\
		    1  & (\mbox{if } W^T \circ x - b) \geq 0 \\
	    \end{array} \right.\label{eq:neural_networks/perceptron_func}
\end{equation}
Equation~(\ref{eq:neural_networks/perceptron_func}) is the mathematical definition of the perceptron unit. In
the case in which the output space is not binary, then the following mathematical model is employed,
\begin{equation}
    y = W^T \circ x + b.
    \label{eq:neural_networks/perceptron}
\end{equation}

By itself, the perceptron unit is not too useful, as a random set of weights and bias values are unlikely to
generate reasonable outputs for the particular task to which the model is applied. In order to make the
perceptron model useful, its parameters need to be tuned accordingly, which implies that some kind of
regression needs to be applied to our model. Before discussing how to tune the parameters of the model, we
need to assume that our particular task, abstractly represented with the label $T$, contains a true underlying
population mapping $t$ that encodes the fundamental information needed to solve the problem: the mapping
connects input variables $x$ with their expected output variables $y$. In the simplest case, the output
variables represent binary choices, but more complex output spaces can also be modelled by
eq.~(\ref{eq:neural_networks/perceptron}). Mathematically, the mapping $t$ is defined as the following
morphism:
\begin{equation}
    t: X \to Y,
\end{equation}
where $X$ is the space of all possible inputs $x$, and $Y$ represents the space of target outputs. As a
result, $y = t(x)$. In this context, the perceptron model defined in eq.~(\ref{eq:neural_networks/perceptron})
serves as a parametric model for the mapping $t$, which we denote with the label $\hat{t}_W$. It is common to
encapsulate both weights and bias in a model under the same label, $W$. Additionally, the terms learnable
parameters and weights are usually employed to refer to all the tunable parameters in a particular model. A
particular choice of weights and bias in a model is usually called a configuration.

The goal is then to find the values of $W$ and $b$ that make the perceptron unit approximate the mapping $t$:
$\hat{t}_W \simeq t$. This would enable us to process any input $x$ in order to produce its expected output;
the model would be making predictions. A priori, we cannot know the appropriate configuration. However, we can
try finding it using real examples of the mapping. A collection of inputs $x_T$ whose corresponding outputs
$y_T = t(x_T)$ are known in advance is known as a training set. The training pairs can be collected from
experiments, simulations or other sources. Note that the training set should contain a large enough collection
of pairs $(x_T, y_T)$ such that it represents a reliable approximation of the target mapping $t$. Once the
training set is defined, we can try to find the model's configuration that reproduces the training set with
large accuracy, that is, the configuration of the model that holds
\begin{equation}
    y_T \simeq \hat{t}_W(x_T)\quad \text{for all}\quad x_T.
\end{equation}
However, we should be cautious, as the model might overfit the training dataset, that is, it can learn the
values of $W$ and $b$ that mimic the training set heuristically without learning the fundamental features of
the mapping. Overfitting is common when the model contains a huge number of tunable parameters. However, it
can be controlled by numerous techniques~\cite{ying2019overview}.

Using labelled training pairs $(x_T, y_T)$ in order to find the model's configuration that approximates the
mapping $t$ is usually referred to as a supervised machine learning problem. In contrast, learning the correct
model's configuration from just the input data $x_T$ is called unsupervised learning. In this document, we
only discuss the framework of supervised learning.

Although the perceptron model can be employed in some simple tasks, it has a fundamental problem: it can only
be applied to problems whose output spaces $Y$ are linearly separable. Consequently, problems with non-linear
target mappings cannot be solved with our current model. To circumvent this problem, we can add some
artificial non-linearity to the model in eq.~(\ref{eq:neural_networks/perceptron}):
\begin{equation}
    y = f(W^T \circ x + b).
    \label{eq:neural_networks/nonlinear_perceptron}
\end{equation}
The function $f$ in eq.~(\ref{eq:neural_networks/nonlinear_perceptron}) is an element-wise non-linear
function, called activation function. In this context, element-wise means that the function acts independently
on each input variable:
\begin{equation}
   f(x) = f(x_1, x_2) = [f(x_1), f(x_2)].
\end{equation}
Some commonly used activation functions are: sigmoid functions, hyperbolic tangents and rectified linear units
(ReLU). A comprehensive list of common activation functions can be found in Reference~\cite{PyTorch} or
\href{https://pytorch.org/docs/stable/nn.html}{inside this link}.

The non-linear perceptron model is still not flexible enough to learn any desired target mapping. In order to
generate more flexible models, we can stack multiple perceptron units; a collection of non-linear perceptron
units is usually called a neural network layer, and each perceptron model in a layer is usually called a
neuron. In general, all neurons in a layer share the same input, but each has its own independent set of
weights and biases. Figure~(\ref{fig:neural_networks/layer}) contains a diagram representing a neural network
layer composed by $3$ neurons. 

\begin{figure}[H]
	\caption{
        Diagram representing a layer composed by stacking $3$ neurons. The layer takes
        $2$-dimensional inputs, and produces a $1$-dimensional output. The output of each neuron is
        fed to an activation function $f$, shared among all $3$ neurons.
    }\label{fig:neural_networks/layer}
	\centering
    \begin{tikzpicture}
        % Input layer
        \foreach \x in {1, ..., 2} {
            \draw node at (0.0, - \x * 1.25) [node, fill=TikzGreen!30] (input_\x) {$x_{\x}$};
        };

        \foreach \x in {1, ..., 3} {
            \draw node at (3.0, - \x * 1.25 + 0.625) [node, fill=TikzOrange!30] (neuron_\x) {$a_{\x}$};
        };

        % Add the connections in the diagram
        \foreach \i in {1, ..., 2} {
            \foreach \j in {1, ..., 3} {
                \draw [arrow] (input_\i) to (neuron_\j);
            };
        };

        % Draw the output of the layer
        \draw node at (6.0, - 2 * 1.25 + 0.625) [node, fill=TikzYellow!30] (output) {$A(x)$};
        \foreach \i in {1, ..., 3} {
            \draw [arrow] (neuron_\i.east) -- (output);
        }

        % Add some text to the figure
        \node at (3.0, 0.25) {$a_i = f(\sum_{j} W_{ij} x_j + b_i)$};
        \node at (0.0, -4.0) {$x$};
        \node at (3.0, -4.0) {$A$};
    \end{tikzpicture}
\end{figure}

Neurons are non-linear models, which implies that their output is usually passed through an element-wise
activation function, which is typically shared among all neurons forming the layer. The mathematical model of
a simple linear neural network linear containing $N_n$ neurons is similar to that of the non-linear perceptron
unit defined in eq.~(\ref{eq:neural_networks/nonlinear_perceptron}), and reads
\begin{equation}
    A = f(Z) = f(x \circ W + b).
    \label{eq:neural_networks/layer}
\end{equation}
In a neural network layer, $W$ represents a matrix of dimensions $\text{dim}[W] = (N_x, N_n)$, the input
vector $x$ has dimensions $\text{dim}[x] = (1, N_x)$, and $b$ holds $\text{dim}[b] = (1, N_n)$. As a result,
the output $A$, sometimes called the layer's activation, has dimensions $(1, N_n)$.

The linear neural network layer defined in eq.~(\ref{eq:neural_networks/layer}) is called a feedforward layer.
The adjective feedforward implies that the information only flows from input to output: the layer does not
contain any loops nor backwards connections between neurons. In principle, non-feedforward layers can also be
built and employed, however, training them tends to be difficult. This chapter only discusses feedforward
neural networks.

\begin{figure}[H]
	\centering
	\caption{
        Diagram representing a feedforward neural network composed by $4$ layers; three hidden
        layers and one output layer. Each of the $4$ layers contains an independent set of weights,
        with dimensions $(N_{n}^{l-1}, N_{n}^{l})$, and an independent set of bias, with dimensions
        $(N_{n}^{l})$. Each layer processes its input according to
        eq.~(\ref{eq:neural_networks/layer}).
    }\label{fig:neural_networks/neural_network}
    \begin{tikzpicture}

        % Input layer
        \foreach \x in {1, ..., 4} {
            \draw node at (0.0, - \x * 1.25) [node, fill=TikzGreen!30] (input_\x) 
                {$x_{\x}$};
        };

        % First hidden layer
        \foreach \x in {1, ..., 5} {
            \draw node at (2.0, -\x * 1.25 + 0.625) [node, fill=TikzYellow!30] (hidden1_\x)
                {$a^{1}_{\x}$};
        };

        % Second hidden layer
        \foreach \x in {1, ..., 5} {
            \draw node at (4.0, -\x * 1.25 + 0.625) [node, fill=TikzYellow!30] (hidden2_\x)
                {$a^{2}_{\x}$};
        };

        % Third hidden layer
        \foreach \x in {1, ..., 3} {
            \draw node at (6.0, -\x * 1.25 - 0.625) [node, fill=TikzYellow!30] (hidden3_\x)
                {$a^{3}_{\x}$};
        };

        % Output layer
        \foreach \x in {1, ..., 2} {
            \draw node at (8.0, -\x * 1.25 - 2 * 0.625) [node, fill=TikzOrange!30] (output_\x)
                {$y_{\x}$};
        };

        % Connections from input to hidden_1
        \foreach \i in {1, ..., 4} {
            \foreach \j in {1, ..., 5} {
                \draw [arrow] (input_\i) to (hidden1_\j);
            };
        };

        % Connections from hidden_1 to hidden_2
        \foreach \i in {1, ..., 5} {
            \foreach \j in {1, ..., 5} {
                \draw [arrow] (hidden1_\i) to (hidden2_\j);
            };
        };

        % Connections from hidden_2 to hidden_3
        \foreach \i in {1, ..., 5} {
            \foreach \j in {1, ..., 3} {
                \draw [arrow] (hidden2_\i) to (hidden3_\j);
            };
        };

        % Connections from hidden_3 to output
        \foreach \i in {1, ..., 3} {
            \foreach \j in {1, ..., 2} {
                \draw [arrow] (hidden3_\i) to (output_\j);
            };
        };
    \end{tikzpicture}
\end{figure}

A complete feedforward linear neural network can be built by connecting $N_L$ different layers. Each layer is
labelled with an index $l$ and contains $N_{n}^{l}$ independent neurons; the number of neurons in each layer
is arbitrary, and should be tuned for the task in question. The output of each layer is usually passed to an
activation function $f^l$, which can be different for each layer in the network. In a feedforward network
composed by $N_L$ layers, there are $N_L - 1$ \textit{hidden layers} whose output is not visible. The last
layer produces the output of the neural network, $y = \hat{t}_W(x)$. A neural network can be viewed as a set
of non-linear transformation applied sequentially over the input space. A diagram showing a complete
feedforward neural network with $4$ layers can be found in Figure~(\ref{fig:neural_networks/neural_network}).

Feedforward neural networks composed by several connected layers are known to be universal approximators, that
is, they are able to approximate any arbitrary mapping with any desired precision;
see~\cite{hornik1989multilayer} and references therein for a proof of this property. However, note that
different architectures might be able to exploit the subtleties in the data better than other to enhance the
convergence of the model to the solution. Furthermore, perfect convergence is usually hindered by the
unavoidable uncertainties present in the data, the lack of adaptability of the model employed, or problems
related to finding the correct solution numerically. 

In a neural network composed by multiple layers, each sequential layer in the network helps the model extract
the relevant features of the mapping, which implies that, in general, the deeper the network, the better. In
the field of neural networks, the adjective deep implies that the model contains numerous connected layers.
Deep neural networks tend to be computationally expensive to train as they contain huge amount of tunable
parameters. Additionally, training deep neural is inherently difficult as they suffer the so-called vanishing
and exploding gradients problems~\cite{glorot2010understanding}. Nevertheless, deep neural networks are
nowadays fruitfully applied to a variety of complex tasks~\cite{samek2021explaining, canziani2016analysis}.

The neural network mathematical model defined in eq.~(\ref{eq:neural_networks/layer}) allows processing
several input examples simultaneously. To see this, imagine that we had a training dataset composed by $N_b$
input pairs, $x$. We can stack all the inputs into a matrix of dimensions $(N_b, N_x)$, and directly apply
eq.~(\ref{eq:neural_networks/layer}) on the input matrix. In this case, the output of a neural network layer
is another matrix of dimensions $(N_b, N_n)$, where $N_n$ is the number of neurons in the layer.
 
The topology of a neural network is sometimes called its architecture. Different architectures can be
generated by varying the number of hidden layers, the number of neurons in each layer, and the activation
functions. Moreover, complex architectures can be built by modifying the mathematical model in
eq.~(\ref{eq:neural_networks/layer}), or the connections between layers.
 
\subsection{Training neural networks}
We are now in the position of discussing how neural networks are trained to solve real-world problems. As
stated before, training is just a synonym for learning the correct neural network configuration so that a
target mapping $t$ is correctly reproduced. Equivalent to the perceptron model, the neural network acts as a
parametrisation of target mapping: $\hat{t}_W$.

Understanding the properties of the particular task to which a neural network is being applied allows us to
decide the best architecture for the problem in question. However, it is important to note that, a priori, we
cannot know whether a particular architecture will perform better or worse than other architecture on a given
task. As a result, it is advisable to explore the available literature in order to find benchmarks on similar
tasks before training a possibly non-optimal architecture.

In our supervised learning framework, we assume that the input space $X$ and the output space $Y$ can be
treated as probability spaces. As a result, the training set corresponds to a sample of both input and output
spaces. The network will learn from those samples in order to find a configuration that approximates the
target mapping $t$. In order to learn the correct mapping from the data, we use a penalty-based approach: the
network will receive a penalty every time it generates a mapping $\hat{t}_W(x)$ that does not lead to the
correct expected outcome, that is, $y_T$. This penalty is usually measured in terms of a loss/cost function:
\begin{equation} 
    \mathcal{L} = \mathcal{L}(y_T = t(x_T), \hat{y}_T = \hat{t}_W(x_T)).
\end{equation} 
Loss functions are scalar functions that measure some kind of distance between the target mapping, $t(x)$, and
the current neural network approximation of that mapping, $\hat{t}_W(x)$. Several loss functions are available
depending on the task in question. For example, in classification problems, the most common loss function is
the so-called cross-entropy loss, which is derived from information theory, and measures the distance between
two probability distribution functions. In contrast, in regression problems, the mean squared error (MSE)
function is typically employed, although other options are also available, such as the $L1$-loss function or
the Huber loss function. A list of common loss functions and their definitions can be found in
Reference~\cite{PyTorch} or \href{https://pytorch.org/docs/stable/nn.html}{inside this link}.

Typically, training a neural network on a particular task using supervised learning needs several prior
choices, such as: the network architecture, the loss function, and the initial values of the learnable
parameters. Additionally, the values of some other hyperparameters might need to be chosen accordingly. 

In our supervised framework, the optimal values of the learnable parameters, labelled $\tilde{W}$, are those
that fulfil the following condition,
\begin{equation}
    \tilde{W} = \argmin_{W} \mathcal{L}(y_T = t(x_T), \hat{Y}_T = \hat{t}_W(x_T)).
    \label{eq:neural_networks/argmin_eq}
\end{equation} 
The optimal parameters depend on the loss function used, and they represent the configuration that makes the
neural network mapping approximate the true target mapping according to $\mathcal{L}$. In this context,
$\tilde{W}$ is found by minimising the loss function on the training set.

Finding the configuration that minimises eq.~(\ref{eq:neural_networks/argmin_eq}) is equivalent to finding the
points at which the first derivative of $\mathcal{L}$ with respect to $W$ is zero:
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial w}\bigg|_{w = \tilde{w}} = 0
        \quad \text{for all}\quad w \in W
    \label{eq:neural_networks/partial_zero}
\end{equation}

In most cases, eq.~(\ref{eq:neural_networks/partial_zero}) represents an analytically intractable system of
equations. As a result, numerical minimisation methods are required. One of the simplest methods available to
find a solution to eq.~(\ref{eq:neural_networks/partial_zero}) is the so-called gradient descent algorithm, a
first-order numerical algorithm employed to find the minimums of a scalar function $\mathcal{L}$. In gradient
descent, the arguments of the function are updated iteratively until a minimum is found. Note that the
solution might correspond to a local minimum; convergence to a global minimum is not ensured in gradient
descent. As in any numerical minimisation algorithm, initial values for the function arguments are needed. The
selected initial values might affect the performance of the algorithm.

In gradient descent, at each iteration, we update each function argument depending on the change of
$\mathcal{L}$hen each individual learnable parameter $w$ is infinitesimally modified. The variation of
$\mathcal{L}$ approximated using the Taylor expansion of $\mathcal{L}$ at first order,
\begin{equation}
    \Delta \mathcal{L} = \mathcal{L}(w + \delta w) - \mathcal{L}(w) \simeq
        \vec{\nabla}\mathcal{L}(w)\big|_{w = w + \delta x} \delta w,
    \label{eq:neural_networks/variation}
\end{equation} 
where $\vec{\nabla}$ is the gradient operator, and $\delta w$ is usually called the learning rate. The
learning rate is typically a constant labelled $\alpha$.
  
The minimum of $\mathcal{L}$ is found when $\Delta \mathcal{L}$ is close to zero. As a consequence, if we
update each weight iteratively using
\begin{equation}
    w^{i + 1} = w^{i} - \alpha \cdot \vec{\nabla} \mathcal{L}\big|_{w^{i}},
    \label{eq:neural_networks/gradient_descent}
\end{equation} 
eventually, we will arrive at a local minimum of $\mathcal{L}$, as gradient descent updates the function
arguments in the opposite direction in which $\mathcal{L}$ grows.

Note that each weight in the neural network needs to be updated at every iteration. Consequently,
the derivatives of the loss function with respect to all learnable parameters in the model are
required. We can compute them using the chain rule. For example, the derivative of $\mathcal{L}$
with respect to a given weight located in the layer $l$ of a model with $L$ total layers can be
computed using
\begin{equation} 
    \frac{\partial \mathcal{L}}{\partial w} = 
        \frac{\partial \mathcal{L}}{\partial \hat{t}_W(x_T)} 
        \frac{\partial \hat{t}_W(x_T)}{\partial A^{L-1}} 
        \frac{\partial A^{L-1}}{\partial Z^{L-1}} 
        \frac{\partial Z^{L-1}}{\partial A^{L-2}} 
        \cdots 
        \frac{\partial Z^{l}}{\partial w}, 
\end{equation} 
where $w^l$ represents a particular parameter of the model, $Z^l$ is the affine transformation at
the layer $l$, defined as
\begin{equation}
    Z^l = W^l \circ A^{l-1} + B^l,
\end{equation}
and $A^l = f^l(Z^l)$ is its non-linear activation. The derivatives are easily computable at every
stage as we know the functional forms of both $Z$ and $A$ at every layer. At each training
iteration, each parameter in the model is updated according to
eq.~(\ref{eq:neural_networks/gradient_descent}):
\begin{equation} 
    w_{i + 1} = w_{i} - \alpha \cdot \frac{\partial \mathcal{L}}{\partial w}\bigg|_{w_i}. 
    \label{eq:neural_networks/weight_update}
\end{equation}
In practice, the derivatives are automatically computed using the framework of automatic
differentiation (AD)~\cite{paszke2017automatic, baydin2018automatic}.

Nowadays, there exist a wide range of variations of the standard gradient descent algorithm with
enhanced properties. Some of those variations are: the stochastic gradient descent algorithm (SGD),
momentum-based gradient descent, or the ADAM algorithm. More information about gradient descent and
some of its variations can be found in Reference~\cite{ruder2016overview}. Additionally, note that
eq.~(\ref{eq:neural_networks/variation}) is a first-order approximation of the total variation. Some
algorithms use second order approximations of $\Delta \mathcal{L}$, which need to approximate the
Hessian operator. The Hessian operator is the matrix containing all the second-order derivatives of
$\mathcal{L}$ with respect to $W$. As a result, second-order optimisation algorithms tend to be
computationally expensive.

The standard algorithm used to train neural networks is called forward/backward propagation. The
algorithm is divided into two different stages: the forward propagation, and the backward
propagation. In the first stage, the network produces its current estimate of the population mapping
$t$ for some training inputs $x_T$. After computing the model's output, $\hat{y} = \hat{t}_W(x_T)$,
we measure the distance between $\hat{y}$ and the expected true training outputs $y_T$ according to
a previously chosen loss function. Once the loss function is estimated on the training set, we
proceed to the second stage, the backward propagation. In this part of the algorithm, the first
derivatives of the loss function with respect to all learnable parameters in the model are
calculated. Once the gradients are computed, we perform an update of all parameters in the model
according to eq.~(\ref{eq:neural_networks/weight_update}).

Due to the fact that the in the first stage of the algorithm the information flows from input to
output, and in the second stage the information flows backwards, training a neural network is
usually represented as a continuous loop: the forward-backward training loop.
Figure~(\ref{fig:neural_networks/backpropagation}) contains a diagram representing the standard
iterative algorithm used to train neural networks.

\begin{figure}[H]
    \centering
    \caption{
        Diagram showing the standard forward-backward propagation algorithm used to train a neural
        network. In the forward propagation step, the network produces the current estimate of the
        population mapping $\hat{y} = \hat{t}_W(x_T)$. This estimate is employed to compute the
        empirical loss function at the current iteration using the expected training output, $y_T$.
        In the back-propagation step, the gradient of the empirical loss with respect to all
        parameters of the network is computed. Those gradients are used in
        eq.~(\ref{eq:neural_networks/weight_update}) to update the current network configuration.
        The loop is repeated until training is finished.
    }\label{fig:neural_networks/backpropagation}
    \begin{tikzpicture}

        % -- Draw the network
        \draw node at (0.0, 0.0)  [node, fill=TikzGreen!30]  (input)    {$x_T$};
        \draw node at (4.0, 1.0)  [node, fill=TikzYellow!30] (hidden_1) {$a^1_1$};
        \draw node at (4.0, -1.0) [node, fill=TikzYellow!30] (hidden_2) {$a^1_2$};
        \draw node at (8.0, 0.0)  [node, fill=TikzOrange!30] (output)   {$\hat{y}$};

        % -- Draw the connections
        \draw [arrow] (input) to (hidden_1);
        \draw [arrow] (input) to (hidden_2);
        \draw [arrow] (hidden_1) to (output);
        \draw [arrow] (hidden_2) to (output);

        % -- Draw the forward pass line
        \draw [arrow, bend right, ultra thick, draw=TikzGreen]  (0.0, -1.0) to (8.0, -1.0);
        \draw [arrow, bend right, ultra thick, draw=TikzOrange] (8.0, 1.0) to (0.0, 1.0);

        % -- Write down the equations names
        \node at (4.0, -2.5) {Forward pass: $\mathcal{L}(\hat{t}_W(x_T), y_T)$};
        \node at (4.0,  2.5) {Backward pass: $\frac{\partial \mathcal{L}}{\partial W}$};
    \end{tikzpicture}
\end{figure}

Training a neural network model using back-propagation is an iterative process. The network updates
its parameters several times employing the same training set. As a consequence, the network revisits
the same dataset at training several times. Every time the network predicts the output of all inputs
in the training datasets, we say that the network has performed a training epoch. In general,
training is carried out for many epochs. Choosing the right number of epochs to train a network is
not straightforward, as numerical optimisation methods are not ensured to reach a global minimum. As
a consequence, networks tend to be trained for several epochs, until their performance plateaus.
Additionally, while training, it is standard to test the performance of the model in a test set,
that is, a collection of inputs whose outputs are known, but not included at training.

Due to the fact that training sets tend to contain large number of examples, it is common to split
the training set into mini-batches to reduce the computational cost. In addition, this can also help
over-fitting, as the neural network updates its parameters at every iteration using only a
proportion of the total dataset.

Some resources discussing neural networks, how to implement and train them, and their possible
applications can be found in References~\cite{goodfellow2017deep, ketkar2017deep,
nielsen2015neural}.
 
\section{Convolutional neural networks} 
Feedforward linear networks are perform well in almost all tasks, however, they assume that the
input space is ultra-local, that is, each variable in the input space is completely independent.
This assumption is reasonable for unstructured data belonging to a vector space, where no spatial
connections between different variables exist. However, this condition is not always encountered in
all types of data, for example, in images, time series measurements or functional data structures.
In all these examples, a given discrete input $x$ of dimension $L$ contains points whose values
depend on their neighbourhood, for example, the colour of a pixel in an image depends on its
surroundings. As a result, a modified version of the standard linear neural network layer defined in
eq.~(\ref{eq:neural_networks/layer}) that exploits this property could lead to improved performances
for datasets with correlated input spaces.
 
We can exploit the locality in our data by making use of the concept of neighbourhood. For this to
work, the input space should be measurable: there must be a notion of closeness between inputs. For
instance: if the input space corresponds to images, the neighbourhood of a particular pixel is
defined as all pixels in an area surrounding that pixel; in time-series data, the neighbourhood of a
measurement is composed by all points in a region surrounding that particular measurement. Locality
can be exploited when there is a notion of \textit{volume} in the input data structures.

The goal is then to modify the model in eq.~(\ref{eq:neural_networks/layer}) so that it takes into
account the local information in the input in order to produce an output. A way of achieving this is
by using convolutions. Fundamentally, a convolution is just an operator that applies a function,
usually called kernel or filter, over an input function. We use the label $K$ to refer to kernels,
while the label $F$ denotes input functions. In general, convolutions are defined as
\begin{equation}
    F^\prime(t) = (F \star K)(t)  = \int_{-\infty}^\infty\ ds F(t) K(s - t),
\end{equation}
where $t$ represents the arguments of the function $F$. The operator $\star$ is usually employed to
denote convolutions.

In our particular case, we are interested in discrete convolutions, that is, convolutions of
discrete signals $F$ with discrete kernels $K$:
\begin{equation} 
    F^\prime[t] = (F \star K)[t]  = \sum_s F[s] K[s - t].
    \label{eq:neural_networks/conv_discrete}
\end{equation} 
The notation $[t]$ implies that we are accessing the $t^{\rm th}$ element in the signal. The sum
runs over all the possible values in which $F$ is defined. The action of $K$ over $F$ at a given
position $t$ selects a neighbourhood of $F$ centred at $t$, and produces a weighted average of the
input signal, where the weights are assigned by $K$. An example of a convolution over a
$1$-dimensional signal using a bell-shaped kernel can be found in
Figure~(\ref{fig:neural_networks/conv1d}). The bell-shaped kernel used in
Figure~(\ref{fig:neural_networks/conv1d}) belongs to the parametric family of Gaussian kernels,
defined as
\begin{equation} 
    K(s - t; \sigma) = \exp(- {\frac{{(s - t)}^2}{\sigma^2}}),
    \label{eq:neural_networks/gaussian_kernel}
\end{equation}
where $\sigma$ is a free-parameter completely specifying $K$. A value of $\sigma = 2$ is employed in
Figure~(\ref{fig:neural_networks/conv1d}).

\begin{figure}[t]
    \caption{ 
        An example of a $1$-dimensional convolution of a time series signal with a bell-shaped
        kernel $K$. In this case, the convolution takes place in a window of $20$ measurements. The
        upper figure shows the original signal and the kernel acting on a particular region through
        eq.~(\ref{eq:neural_networks/conv_discrete}). The lower figure shows the result of the
        convolution.
    }\label{fig:neural_networks/conv1d}
    \begin{center}
        \includegraphics[width=0.75\textwidth]{./content/NeuralNetworks/assets/1dconv.pdf}
    \end{center}
\end{figure}

In order to generate a neural network layer that exploits the locality in the input signal, we
promote the convolution operation in eq.~(\ref{eq:neural_networks/conv_discrete}) as the new
\textit{affine transformation} $Z$ in eq.~(\ref{eq:neural_networks/layer}). This implies that a
convolutional layer will process an input signal $A^{l-1}$ using
\begin{equation} 
    Z^{l} = (A^{l-1} \star K)[t]  = \sum_s A^{l-1}[s] K^{l}[s - t] + b^{l},
\end{equation}
where $b^{l}$ is an optional bias, and $K^{l}$ is a learnable kernel defining the convolutional
layer. To apply non-linearity to the layer, $Z^{l}$ is usually passed through a non-linear
activation function: $A^{l} = f^{l}(Z^{l})$.
 
The question now is how we define the trainable kernel on each layer, which is equivalent to
selecting the dependence of $K$ with some learnable parameters. One possibility of doing this would
be to use a parametric family of kernels, such as the Gaussian kernels defined in
eq.~(\ref{eq:neural_networks/gaussian_kernel}). Through back-propagation on a training set, the
parameters defining the kernel are tuned to approximate the target mapping. The main problem of this
approach is its lack of flexibility: we cannot be sure that a given parametric family is optimal for
the task in question. To avoid this problem, we can promote $K$ to be a sequence of tunable real
parameters that can be updated at every iteration. In this formulation, $K$ can be viewed as a blank
canvas that can be filled with different values to suit a particular task. This means that the space
of available kernels is richer, at the cost of increased number of learnable parameters in the
model. An example of a blank canvas kernel for $1$-dimensional data with length $8$ is
\begin{equation}
    K = (k_1, k_2, k_3, k_4, k_5, k_6, k_7, k_8), 
\end{equation}
where each of the weights $k_i$ is a tunable real number.
 
Note that for finite signals, convolutions are not ensured to preserve lengths: the output signal
might have a smaller length than the input signal. In general, the convolution of a signal of length
$L$ and a kernel of size $K_x$ produces another signal with length 
\begin{equation}
    L^\prime = L - K_x + 1.
\end{equation}
In order to preserve lengths, it is common to pad the input signal with dummy values both at the
beginning and the end of the signal. The convolution of a padded input signal $F$ with $p$ dummy
values appended both at the beginning and the end of the signal, and a kernel of size $K$, produces
another signal with length
\begin{equation}
    L^\prime = L - N_k + 1 + 2 \cdot p.
    \label{eq:neural_networks/length_shrink}
\end{equation}
In addition, it is sometimes common to skip sequential applications of the kernel over the input to
compress the input signal; this is called striding the convolution. It is worth noting that
convolutions allow the extraction of the fundamental features of the input signal at the cost of
lowering its resolution. Figure~(\ref{fig:neural_networks/convdiagram}) shows an example of a
convolution between a $1$-dimensional input signal of size $L = 4$ and a kernel of size $K_x = 3$.

\begin{figure}[H]
    \caption{
        Example of a convolution between an input signal $F$ of size $L = 4$, and a blank kernel of
        size $K_x = 3$. The output signal has size $L^\prime = L - K_x + 1 = 3$. The convolution is
        applied by superposing the kernel over the signal starting from the beginning, applying the
        convolution operation defined in eq.~(\ref{eq:neural_networks/conv_discrete}), and then
        sliding the window to the next point in the signal. In the case in which stride was used,
        then the window would be moved skipping $s$ points between sequential applications.
    }\label{fig:neural_networks/convdiagram}
    \centering
    \vspace{0.5em}
    \begin{tikzpicture}

        % -- Add some nodes with data
        \node[] at (-1.5, 0.0)  {$F$};
        \node[] at (-1.5, -1.0) {$\star$};
        \node[] at (-1.5, -2.0) {$K$};
        \node[] at (5.5,  -1.0) {$=$};

        % -- Generate the signal and the kernel
        \directlua{
            signal, kernel = {}, {}
            for i = 1, 5 do signal[i] = math.random() end
            for i = 1, 3 do kernel[i] = math.random() end
        }
        
        % -- Draw the signal and the kernel
        \foreach \x in {0, 1, 2, 3, 4} {
            \node[
                box, fill=\directlua{tex.print("TikzGreen!" .. math.floor(100 * signal[\x + 1]))}
            ] at (\x, 0) {
                $\luaexec{tex.print(string.format("\%.2f", signal[\x + 1]))}$
            };
        };
        \foreach \x in {0, 1, 2} {
            \node[
                box, fill=\directlua{tex.print("TikzYellow!" .. math.floor(100 * kernel[\x + 1]))}
            ] at (\x + 1, -2.0) {
                $\luaexec{tex.print(string.format("\%.2f", kernel[\x + 1]))}$
            };
        };

        % -- Draw the output of the signal
        \foreach \x in {0, 1, 2} {
            \node[box] at (\x + 7, -1.00) {
                $\luaexec {
                    local value = 0
                    for i = \x, \x + 2 do
                        value = value + signal[\x + 1]  * kernel[\x + 1]
                    end
                    tex.print(string.format("\%.2f", value))
                }$
            };
        }

        % -- Draw some guidelines on the application
        \draw[<->, draw=TikzOrange, very thick] (-0.5, -0.75) -- (2.5, -0.75);
        \draw[<->, draw=TikzGreen, very thick]  (+0.5, -1.00) -- (3.5, -1.00);
        \draw[<->, draw=TikzRed, very thick]    (+1.5, -1.25) -- (4.5, -1.25);

        \draw[draw=TikzOrange, very thick] (6.5, -0.50) rectangle ++(1.0, -1.0);
        \draw[draw=TikzGreen, very thick]  (7.5, -0.50) rectangle ++(1.0, -1.0);
        \draw[draw=TikzRed, very thick]    (8.5, -0.50) rectangle ++(1.0, -1.0);

    \end{tikzpicture}
\end{figure}

Convolutions can also be applied to higher dimensional data, for example, images. In grey-scale
image-processing, images are represented as matrices of dimensions $(L_x, L_y)$ whose entries
correspond to different pixels. Each pixel in the image represents a shade of grey. In this
$2$-dimensional context, kernels can be represented as $2$-dimensional functions.

Applying a kernel $K$ to an image $I$ can be done through the $2$-dimensional generalisation of
eq.~(\ref{eq:neural_networks/conv_discrete}):
\begin{equation}
    I^\prime_{x, y} = {(I \star K)}_{x, y}  = \sum_{i, j} I_{x - i, y - j} K_{i, j},
    \label{eq:neural_networks/image_conv}
\end{equation}
where the entries of $K$ dictate how $I$ would transform under the convolution.
Figure~(\ref{fig:neural_networks/filter}) contains a diagram where convolutions are employed to
select the edges of a grey-scale image. This is a common image processing technique included in most
image manipulation programs.
 
\begin{figure}[H]
    \caption{
        Diagram showing the convolution operation of a simplified binary $4 \times 4$ image with a
        $2 \times 2$ edge-selection filter. The resulting image shows the regions in which the
        original image contain edges.
    }\label{fig:neural_networks/filter}
    \centering
    \begin{tikzpicture}

        % -- Draw the image
        \foreach \x in {0, 1, 2, 3} {
            \foreach \y in {0, 1, 2, 3} {
                \node[box] at (\x, \y) {$\color{TikzBlue} 0$};
            }
        }

        % -- Colour some of the grids
        \foreach \x in {0, ..., 3} \node[box, fill=TikzBlue!30] at (\x, 0) {$\color{TikzBlue} 1$};
        \foreach \x in {1, ..., 3} \node[box, fill=TikzBlue!30] at (\x, 1) {$\color{TikzBlue} 1$};
        \foreach \x in {2, ..., 3} \node[box, fill=TikzBlue!30] at (\x, 2) {$\color{TikzBlue} 1$};
        \foreach \x in {2, ..., 3} \node[box, fill=TikzBlue!30] at (\x, 3) {$\color{TikzBlue} 1$};

        % -- Add the convolution operator
        \node[] at (4.0, 1.5) {$\star$};

        % -- Draw the kernel
        \node[box] at (5.0, 1.0) {$\color{TikzBlue} -1$};
        \node[box] at (5.0, 2.0) {$\color{TikzBlue} -1$};
        \node[box] at (6.0, 1.0) {$\color{TikzBlue} +1$};
        \node[box] at (6.0, 2.0) {$\color{TikzBlue} +1$};

        % -- Add the equal sign operator
        \node[] at (7.0, 1.5) {$=$};

        % -- Draw the resulting image
        \foreach \x in {0, 1, 2} {
            \foreach \y in {0, 1, 2} {
                \node[box] at (8 + \x, \y + 0.5) {$\color{TikzBlue} 0$};
            }
        }

        % -- Draw the special edges
        \node[box, fill=TikzGreen!30] at (8 + 0.0, 0.0 + 0.5) {$\color{TikzBlue} 1$};
        \node[box, fill=TikzGreen!30] at (8 + 0.0, 1.0 + 0.5) {$\color{TikzBlue} 1$};
        \node[box, fill=TikzGreen!30] at (8 + 1.0, 1.0 + 0.5) {$\color{TikzBlue} 1$};
        \node[box, fill=TikzGreen!30] at (8 + 1.0, 2.0 + 0.5) {$\color{TikzBlue} 2$};

        % -- Draw some guidelines on the operation
        \draw[draw=TikzOrange!80!TikzRed, very thick] (0.5, 3.5) rectangle ++(2.0, -2.0);
        \draw[draw=TikzOrange!80!TikzRed, very thick] (8.5, 3.0)  rectangle ++(1.0, -1.0);

        % -- Add some identifiers to the images
        \node[] at (1.5, -1.25) {$I$};
        \node[] at (5.5, -1.25) {$K$};
        \node[] at (9.0, -1.25) {$(F \star K)$};
    \end{tikzpicture}
\end{figure}

When applied to neural networks, the grey-scale image-processing kernels are matrices of size $(K_x,
K_y)$, whose entries correspond to learnable weights:
\begin{equation} 
    K = \begin{pmatrix} 
        \luaexec{
            local x, y = 3, 3
            for i = 1, x do
                for j = 1, x do 
                    local del = (j ~= y) and " & " or " \\\\ "
                    tex.print(string.format("w_{\%d, \%d} \%s ", i, j, del))
                end
            end
        }
    \end{pmatrix}
    \label{eq:neural_networks/kernel_image}
\end{equation}

Studying how convolutional layers could be applied to coloured images allows us to define an
important concept: the channel or feature space. In coloured images, each pixel represents a
$3$-dimensional vector belonging to colour space. As a consequence, coloured images can be viewed as
tensors of dimensions $(3, L_x, L_y)$, where the first dimension corresponds to the colour space of
each pixel in the image. To process tensorial objects using convolutions, we would need to employ
tensorial kernels. In the case of $2$-dimensional images, the kernel dimensions would be: $(C_o,
C_i, K_x, K_y)$, where $C_o$ is the number of output channels of the layer, and $C_i$ is the number
of channels of the input signal. The output of the layer would be a tensor of dimensions $(C_o,
L_x^\prime, L_y^\prime)$, where both $L_x^\prime$ and $L_y^\prime$ are computed following
eq.~(\ref{eq:neural_networks/length_shrink}). A possible implementation of a convolutional layer in
this case is
\begin{equation} 
    Z^{l}_{c} = b^{l}_c + \sum_{k = 0}^{C_i - 1} K^l_{c, k} \star I_{k}.
    \label{eq:neural_networks/conv_layer}
\end{equation}
In the equation above, $c$ represents a particular output channel, $k$ refers to one input channel,
and $\star$ is the convolution operator defined in eq.~(\ref{eq:neural_networks/image_conv}). A
given output channel is computed by adding several convolutions. The dimension of each output
channel is $(1, L_x^\prime, L_y^\prime)$. This procedure allows us to compensate the loss of
resolution in the input signal produced by the convolutions by increasing the feature/channel space.
Typically, the output of the convolutional layer $Z^{l}$ is fed to a non-linear activation function
in order to add some non-linearity to the output space. Figure~(\ref{fig:neural_networks/convlayer})
contains a diagram showing how convolutions can be applied to coloured images.

\begin{figure}
    \caption{
        Diagram showing how convolutions could be applied to input data with multiple features, that
        is, multiple channels as encountered in coloured images. In this particular case, the input
        could be a coloured image of $4 \times 4$ dimensions, which can be represented by a tensor
        of dimensions $(3, 4, 4)$. As we would like to obtain an output containing $2$ channels, we
        need to employ a tensorial kernel of dimensions $(2, 3, K_x, K_y)$, where $K_x = K_y = 2$.
        The convolved signal is computed using eq.~(\ref{eq:neural_networks/conv_layer}),
        Consequently, its dimensions are $(2, 3, 3)$.
    }\label{fig:neural_networks/convlayer}
    \centering
    \begin{tikzpicture}

        % -- Add some text to the figure
        \node[] at (0.5, -1.25) {$\text{dim}[I] = (3, 4, 4)$};
        \node[] at (5.0, -1.25) {$\text{dim}[K] = (2, 3, 2, 2)$};
        \node[] at (9.5, -1.25) {$\text{dim}[I^\prime] = (2, 3, 3)$};

        % -- Add the 2 dimensional tensor
        \node[] at (2.30, 0.5) {$\star$};
        \node[] at (2.85, 0.5) {$\Bigg{(}$};
        \node[] at (7.35, 0.5) {$\Bigg{)}$};
        \node[] at (5.00, 0.5) {\text{,}};
        \node[] at (8.00, 0.5) {$=$};

        % -- Generate a grid to define each layer of the three dimensional image
        \foreach \x in {0, 1, 2, 3} {
            \foreach \y in {0, 1, 2, 3} {
                \node[small, fill=TikzBlue!45]  at (0.5 * \x,  0.5 * \y) {};
                \node[small, fill=TikzGreen!45] at (0.5 * \x - 0.25,  0.5 * \y - 0.25) {};
                \node[small, fill=TikzRed!45]   at (0.5 * \x - 0.50,  0.5 * \y - 0.50) {};
            };
        };

        % -- Generate the kernel with dimensions (2, 3, 2, 2)
        \foreach \x in {0, 1} {
            \foreach \y in {0, 1} {
                % -- First kernel
                \node[small, fill=TikzYellow!20!TikzOrange!80] at (0.5 * \x + 4,    0.5 * \y + 0.50) {};
                \node[small, fill=TikzYellow!40!TikzOrange!60] at (0.5 * \x + 3.75, 0.5 * \y + 0.25) {};
                \node[small, fill=TikzYellow!60!TikzOrange!40] at (0.5 * \x + 3.50, 0.5 * \y + 0.00) {};

                % -- Second kernel
                \node[small, fill=TikzBlue!20!TikzGreen!80] at (0.5 * \x + 6,    0.5 * \y + 0.50) {};
                \node[small, fill=TikzBlue!40!TikzGreen!60] at (0.5 * \x + 5.75, 0.5 * \y + 0.25) {};
                \node[small, fill=TikzBlue!60!TikzGreen!40] at (0.5 * \x + 5.50, 0.5 * \y + 0.00) {};
            };
        };

        % -- Generate the output of the network
        \foreach \x in {0, 1, 2} {
            \foreach \y in {0, 1, 2} {
                \node [small, fill=TikzBlue!50!TikzGreen!50]    at (0.5 * \x + 9.00, 0.5 * \y + 0.25) {};
                \node [small, fill=TikzYellow!50!TikzOrange!50] at (0.5 * \x + 8.75, 0.5 * \y + 0.00) {};
            };
        };

    \end{tikzpicture}
\end{figure}
 
To conclude our discussion about convolutional neural networks, we note that convolutional layers
can be used in combination with regular linear layers. To do so, we can \textit{flatten} the output
tensors by stacking their outputs into a $1$-dimensional array. Nowadays, it is frequent to include
several convolutional layers in a neural network, mainly in the first stages of the architecture.
After the input is processed by all convolutional layers, a set of linear layers is usually employed
before generating the final output of the network.

Convolutional neural networks produce state-of-the-art results in a large variety of tasks. However,
training them tends to be relatively expensive due to their large number of learnable parameters.
Before applying convolutional layers to a given task, it is important to analyse the properties of
the problem in question. Convolutions are mainly useful when dealing with data structures containing
a notion of locality. For more information about convolutional neural networks, their applications,
and recent developments, we refer to~\cite{yamashita2018convolutional, gu2018recent, li2021survey,
kiranyaz20211d}.
